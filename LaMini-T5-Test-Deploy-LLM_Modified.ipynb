{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4834eb54",
   "metadata": {},
   "source": [
    "# Deploying Large Language Models (LLMs) on AWS Sagemaker ml.g4dn.2xlarge instance\n",
    "\n",
    "This notebook demonstrates  deploying a LLM using the Hugging Face Transformers library. It covers the installation of libraries, loading of pre-trained models and tokenizers, and setting up a  pipeline for text-to-text generation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "938603df-0c16-42d5-b0e9-e2768898bfdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: safetensors, regex, einops, huggingface-hub, bitsandbytes, tokenizers, accelerate, transformers\n",
      "Successfully installed accelerate-0.27.0 bitsandbytes-0.42.0 einops-0.7.0 huggingface-hub-0.20.3 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.1 transformers-4.37.2\n"
     ]
    }
   ],
   "source": [
    "# Library Installation\n",
    "# The following libraries are essential for deploying LLMs:\n",
    "# - `transformers`: Provides access to pre-trained models and utilities for NLP tasks.\n",
    "# - `einops`: Offers flexible and powerful tensor operations.\n",
    "# - `accelerate`: Simplifies running models on multi-GPU setups.\n",
    "# - `bitsandbytes`: Optimizes model training and inference on GPUs.\n",
    "# - `langchain`: (If included) Potentially used for chaining language models or specific NLP tasks.\n",
    "# These libraries form the backbone of our LLM deployment, enabling us to leverage Hugging Face's ecosystem for model handling and optimization.!pip install transformers einops accelerate bitsandbytes langchain\n",
    "!pip install transformers einops accelerate bitsandbytes langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858767c-fb02-4a8b-a77b-6789d5a47bd2",
   "metadata": {},
   "source": [
    "## The first section demonstrates utilizing the LLM on hugging Face without deploying it to AWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b20e2-a3d4-434d-b093-cdf1fb26821d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde7b55-37eb-4019-a6c0-17b8349f45d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"MBZUAI/LaMini-T5-738M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ea43604-050b-4c9c-a411-b0d47f1baddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "def llm_pipeline():\n",
    "    pipe = pipeline(\n",
    "        'text2text-generation',\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 256,\n",
    "        do_sample=True,\n",
    "        temperature = 0.3,\n",
    "        top_p = 0.95\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75b4aaf5-2e4e-45cd-9b82-772b58bd4d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_prompt = \"Write and article on Artificial Intelligence\"\n",
    "model = llm_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13592ac3-6499-4404-bbad-ac3ca6698a69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence, or AI, is a field of computer science that focuses on creating machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI is a rapidly growing field that aims to create machines that can think, reason, and learn like humans. One of the most significant applications of AI is in healthcare. AI is used in various applications, such as image and speech recognition, natural language processing, and predictive analytics. AI algorithms are designed to analyze large amounts of data and identify patterns that can be used to make predictions and recommendations'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = model(input_prompt)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf469545-97a5-497d-95e7-76960d6f99bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The second section demonstrates deploying the LLM to AWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f14a949-3a39-49e9-b97e-bcb4d8e1ef4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You can become more healthy by: 1. Eating a balanced and nutritious diet 2. Regular exercise 3.'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for interacting with AWS services and Hugging Face.\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "# Attempt to get the execution role for SageMaker. This role is needed to give SageMaker access to AWS resources.\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    # If unable to get the execution role directly (e.g., when running outside SageMaker), use boto3 to fetch the IAM role.\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Configuration for the Hugging Face model to be deployed.\n",
    "# This includes the model ID from the Hugging Face Model Hub, the task type, device mapping, and data type settings.\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':model_name,  # The model ID from Hugging Face Hub.\n",
    "    'HF_TASK': 'text2text_generation',  # Specifies the task for the model, here text-to-text generation.\n",
    "    'device_map': 'auto',  # Allows automatic device mapping for model deployment.\n",
    "    'torch_dtype': 'torch_float32'  # Specifies the data type for model tensors.\n",
    "}\n",
    "\n",
    "# Create a Hugging Face Model Class for deployment.\n",
    "# This includes specifying the Docker image URI for the model, environmental variables (model configuration), and the IAM role.\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.8.2\"),  # Get the Docker image URI for Hugging Face.\n",
    "\tenv=hub,  # Pass the model configuration as environment variables.\n",
    "\trole=role,  # IAM role with permissions for SageMaker operations.\n",
    ")\n",
    "\n",
    "# Deploy the model to SageMaker Inference.\n",
    "# This step provisions the necessary infrastructure and deploys the model Docker container.\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,  # Number of instances to start for the deployment.\n",
    "\tinstance_type=\"ml.g4dn.2xlarge\",  # The type of instance to use for the deployment.\n",
    "\tcontainer_startup_health_check_timeout=300,  # Timeout in seconds for the container health check.\n",
    ")\n",
    "  \n",
    "# Send a prediction request to the deployed model.\n",
    "# This sends a sample input to the model and retrieves the prediction.\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"how can I become more healthy?\",  # The input question for the model.\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82894d5e-f13f-48b7-9841-443ccbfe998f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'AWS stands for Amazon Web Services, which is a global network of servers that provide a wide range of cloud computing services and resources to users worldwide. AWS is one of the largest cloud computing services in the world, and it has been instrumental in transforming the way we access, share, and access information. One of the most popular services offered by AWS is the Amazon Web Services (AWS) service. AWS offers a range of services, including web hosting, email services, cloud storage, and mobile apps. AWS has a wide range of features, including real-time updates, real-'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Write short article on AWS\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.7,\n",
    "        \"tempratuere\": 0.3,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"repetition_penality\": 1.03\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "response = predictor.predict(payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f2397-59e9-4693-ae8e-2476fb39205f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
